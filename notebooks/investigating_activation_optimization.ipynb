{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ddd5bf-7615-45a2-bd9c-9827e7c2b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "from sparse_coding_torch.conv_sparse_model import ConvSparseLayer\n",
    "from sparse_coding_torch.small_data_classifier import SmallDataClassifier\n",
    "from sparse_coding_torch.utils import plot_filters\n",
    "from sparse_coding_torch.utils import plot_video\n",
    "\n",
    "from sparse_coding_torch.BamcPreprocessor import BamcPreprocessor\n",
    "from sparse_coding_torch.video_loader import MinMaxScaler\n",
    "from sparse_coding_torch.video_loader import VideoGrayScaler\n",
    "from sparse_coding_torch.video_loader import VideoLoader\n",
    "from sparse_coding_torch.video_loader import VideoClipLoader\n",
    "\n",
    "from sparse_coding_torch.load_data import load_bamc_data\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886cc95c-99cf-425a-8008-bf2b03571193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the devices available and set the batch size\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b9333eb-9984-4233-af8c-2aa24ed6b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 62\n",
    "# video_path = \"/shared_data/bamc_data\"\n",
    "video_path = \"/shared_data/bamc_data_scale_cropped\"\n",
    "\n",
    "# scaled and cropped video size is 400x700\n",
    "transforms = torchvision.transforms.Compose([VideoGrayScaler(),\n",
    "                                             MinMaxScaler(0, 255),\n",
    "                                             Normalize((0.184914231300354,), (0.11940956115722656,)),\n",
    "                                             # BamcPreprocessor(),\n",
    "                                             torchvision.transforms.Resize(size=(200, 350))\n",
    "                                            ])\n",
    "dataset = VideoLoader(video_path, transform=transforms, \n",
    "                          # frames_between_clips=1,\n",
    "                          #num_frames=4\n",
    "                          num_frames=60\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a2897ab-bf2e-4dd8-945f-4190a44230ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.stack([dataset[i][1].float() for i in range(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "912c7b46-da6f-4a16-84cc-31d375261cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.7504), tensor(-1.5486))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.float().max(), data.float().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b0a8048-aabe-4e5e-8c3e-ae1523b9bb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.680491798112143e-09, 0.9966946840286255)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.float().mean().item(), data.float().std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "659e3565-17ca-4e15-b52b-093568b3ef2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.6827057e+07, 7.3490907e+07, 6.7035305e+07, 3.8855535e+07,\n",
       "        1.6149867e+07, 5.5648660e+06, 1.8556760e+06, 5.1971400e+05,\n",
       "        9.6711000e+04, 4.3620000e+03]),\n",
       " array([-1.5485713 , -0.81867284, -0.0887743 ,  0.64112425,  1.3710227 ,\n",
       "         2.1009212 ,  2.8308198 ,  3.5607183 ,  4.290617  ,  5.0205154 ,\n",
       "         5.750414  ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66f764-ff8e-40ad-a008-659a66b27913",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b370f-efd8-41f9-b168-ddc9b1af4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = next(iter(data_loader))\n",
    "example_data[1].shape\n",
    "ani = plot_video(example_data[1][2])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86142a5-930b-4a0c-ab7a-6be63a1d1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(4, 16, 16),\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               lam=0.01,\n",
    "                               max_activation_iter=150,\n",
    "                               activation_lr=1e-2)\n",
    "model = sparse_layer\n",
    "# model = torch.nn.DataParallel(model, device_ids=[1, 0, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d00195-42c4-4e29-9a9e-ada194cc6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"saved_models/sparse_conv3d_model-4x16x16x64-4x200x350-clips.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Put everything on the target device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe54212-0d39-4c5b-a8e9-f3900765934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "inner_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607af8e-7dd1-4bb9-9578-95c279465f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(100)):\n",
    "    epoch_loss = 0\n",
    "    for labels, local_batch, filenames in data_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        # activations = model(local_batch)\n",
    "        activations, inner_loss = model(local_batch)\n",
    "        inner_losses.extend(inner_loss)\n",
    "        \n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "      \n",
    "    epoch_loss /= len(data_loader.sampler)\n",
    "    loss_log.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f90ccc-2246-4d12-8d3f-ae38ad8fcc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(inner_losses[0])\n",
    "plt.plot(inner_losses)\n",
    "plt.ylim(0, 8000)\n",
    "t = 150\n",
    "# plt.xlim(t*150+t, (t+1)*150)\n",
    "plt.xlim(0, 150)\n",
    "print(min(inner_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234fc58-7878-4d0c-bd50-3e73d7182448",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24decdf-d7dc-4a37-8877-e85742ece6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ff08a-ae7d-4bff-8106-a624427d8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38348-6bcb-4dda-8583-4e86fdf8099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][2])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37229a1d-4762-4f3f-a7c1-9b5f98ca637b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "activations = sparse_layer(example_data[1][idx:idx+1].to(device))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7e82c-c90f-4763-ad79-b35c8600486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters(filters):\n",
    "    num_filters = filters.shape[0]\n",
    "    ncol = 8\n",
    "    # ncol = int(np.sqrt(num_filters))\n",
    "    # nrow = int(np.sqrt(num_filters))\n",
    "    T = filters.shape[2]\n",
    "    \n",
    "    if num_filters // ncol == num_filters / ncol:\n",
    "        nrow = num_filters // ncol\n",
    "    else:\n",
    "        nrow = num_filters // ncol + 1\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=ncol, nrows=nrow,\n",
    "                             constrained_layout=True,\n",
    "                             figsize=(ncol*2, nrow*2))\n",
    "\n",
    "    ims = {}\n",
    "    for i in range(num_filters):\n",
    "        r = i // ncol\n",
    "        c = i % ncol\n",
    "        ims[(r, c)] = axes[r, c].imshow(filters[i, 0, 0, :, :],\n",
    "                                        cmap=cm.Greys_r)\n",
    "\n",
    "    def update(i):\n",
    "        t = i % T\n",
    "        for i in range(num_filters):\n",
    "            r = i // ncol\n",
    "            c = i % ncol\n",
    "            ims[(r, c)].set_data(filters[i, 0, t, :, :])\n",
    "\n",
    "    return FuncAnimation(plt.gcf(), update, save_count=filters.shape[2],\n",
    "                         interval=1000/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeaa0cd-987d-452d-b691-a58722a3b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ae8a-cf00-4744-b57c-6fd0fa22735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(5462100, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cf8c8-db9e-4c14-b9f0-418e1893ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                                out_channels=25,\n",
    "                                kernel_size=(20, 16, 16),\n",
    "                                stride=(2, 4, 4),\n",
    "                                padding=0,\n",
    "                                convo_dim=3,\n",
    "                                rectifier=True,\n",
    "                                shrink=0.25,\n",
    "                                lam=0.25,\n",
    "                                max_activation_iter=200,\n",
    "                                activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cb611-36c6-4c39-bd19-fab35d953df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91fc8-3dd6-4f55-9008-5b5a6950c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60afb31-7c80-48e7-a920-80f4cab1e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2caa69-f3dc-4847-8166-68d32220cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b932c9-2c72-449d-a9f2-a246e3ddd325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342754c-5175-40ec-8412-48b457dd360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chris-py3.9.6",
   "language": "python",
   "name": "chris-py3.9.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
