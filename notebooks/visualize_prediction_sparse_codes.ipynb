{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285a1bd-83ef-44b7-9654-012294fd5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from sparse_coding_torch.conv_sparse_model import ConvSparseLayer\n",
    "from sparse_coding_torch.small_data_classifier import SmallDataClassifierConv3d\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sparse_coding_torch.utils import plot_filters\n",
    "from sparse_coding_torch.utils import plot_video\n",
    "\n",
    "from sparse_coding_torch.load_data import load_yolo_clips\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba945d-1794-4d47-b6b5-3577803347d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, _ = load_yolo_clips(batch_size, mode='all_train', device=device, n_splits=1, sparse_model=None)\n",
    "print('Loaded', len(train_loader), 'train examples')\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(5, 15, 15),\n",
    "                               stride=1,\n",
    "                               padding=(0, 7, 7),\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=200,\n",
    "                               activation_lr=1e-1)\n",
    "sparse_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3f6af-1f40-47ed-bf65-ccb0253b3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"/home/dwh48@drexel.edu/sparse_coding_torch/sparse_conv3d_model-pleural_clips2_5x15x15-11-14-21.pt\")\n",
    "sparse_layer.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce2015-183f-4f88-98b2-7cc23790c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_ids = ['image_24164968068436_CLEAN', 'image_73815992352100_clean', 'image_74132233134844_clean']\n",
    "fn_ids = ['image_610066411380_CLEAN', 'image_634125159704_CLEAN', 'image_588695055398_clean', 'image_584357289931_clean', 'Image_262499828648_clean', 'image_267456908021_clean', 'image_2743083265515_CLEAN', 'image_1749559540112_clean']\n",
    "\n",
    "incorrect_sparsity = []\n",
    "correct_sparsity = []\n",
    "incorrect_filter_act = torch.zeros(64)\n",
    "correct_filter_act = torch.zeros(64)\n",
    "\n",
    "for labels, local_batch, vid_f in tqdm(train_loader):\n",
    "    activations = sparse_layer(local_batch.to(device))\n",
    "    sparsity = torch.count_nonzero(activations) / torch.numel(activations)\n",
    "    filter_act = torch.sum(activations.squeeze(), dim=[1, 2])\n",
    "    filter_act = filter_act / torch.max(filter_act)\n",
    "    filter_act = filter_act.detach().cpu()\n",
    "    \n",
    "    if vid_f[0] in fp_ids or vid_f[0] in fn_ids:\n",
    "        incorrect_sparsity.append(sparsity)\n",
    "        incorrect_filter_act += filter_act\n",
    "    else:\n",
    "        correct_sparsity.append(sparsity)\n",
    "        correct_filter_act += filter_act\n",
    "        \n",
    "print(torch.mean(torch.tensor(correct_sparsity)))\n",
    "print(torch.mean(torch.tensor(incorrect_sparsity)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8dfc9-9736-4b1e-bb4b-de6b69056103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b202266-6b44-4c8d-9442-b86e6ad9b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = sparse_layer.filters.cpu().detach()\n",
    "print(filters.size())\n",
    "\n",
    "filters = torch.stack([filters[val] for val in incorrect_filter_act.argsort(descending=True)])\n",
    "\n",
    "print(filters.size())\n",
    "\n",
    "ani = plot_filters(filters)\n",
    "# HTML(ani.to_html5_video())\n",
    "ani.save(\"/home/dwh48@drexel.edu/sparse_coding_torch/incorrect_vis.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1f47c-ba4c-4c3a-9f96-4901c39e16e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pocus_project)",
   "language": "python",
   "name": "darryl_pocus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
